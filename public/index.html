<script src="assets/webcomponents-lite.min.js"></script>
<link rel="import" href="assets/polymer-mini.html">

<script src="assets/d3.min.js"></script>
<script src="assets/d3-scale-chromatic.min.js"></script>

<script>
  function cellClick(cell) {
    document.querySelector('figure-cells[mode=sorted]').setCell(cell);
  }
</script>

<!--
<style>
  .post .paper {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.08);
  }
  .post .row {
    display: flex;
  }
  .post .row .column {
    flex-grow: 1;
    width: 100%;
    margin-right: 18px;
  }
  .post .row .column:last-of-type {
    margin-right: 0;
  }
</style> -->

<link rel="import" href="assets/distill-stage-manager.html">

<!-- <link rel="import" href="assets/figure-gaussian.html"> -->
<!--<link rel="import" href="assets/figure-animated.html"> -->
<link rel="import" href="assets/figure-predict.html">
<!--<link rel="import" href="assets/figure-hairs.html"> -->
<!-- <link rel="import" href="assets/figure-normalize.html"> -->
<!--<link rel="import" href="assets/figure-cells.html"> -->
<!--<link rel="import" href="assets/figure-cells-draw.html"> -->

<style>
  .intro h2 {
    font-weight: normal;
    font-size: 16px;
  }
  @media(min-width: 1024px) {
    .intro h1 {
      max-width: 720px;
      margin-left: auto;
      margin-right: auto;
      text-align: center;
    }
    .intro h2 {
      text-align: center;
      font-size: 20px;
      max-width: 740px;
      margin: 0 auto;
      font-weight: 300;
      line-height: 1.5em;
    }
  }
  .post .controls {
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    margin-top: 48px;
    padding-top: 24px;
  }
  .post .controls span {
    font-size: 14px;
  }
  .post .controls button {
    font-size: 13px;
    padding: 4px 8px;
    border-radius: 3px;
    background-color: rgba(0, 0, 0, 0.03);
    border: 1px solid rgba(0, 0, 0, 0.2);
  }
  .post .controls select {
    margin-right: 8px;
    font-size: 13px;
    font-family: "Open sans";
    border-radius: 3px;
    background-color: rgba(0, 0, 0, 0.03);
    border: 1px solid rgba(0, 0, 0, 0.2);
  }
  .post .controls input[type="range"] {
    position: relative;
    top: 3px;
  }
  .post > h2 {
    margin-top: 60px;
  }
  .post sup {
    vertical-align: baseline;
    position: relative;
    top: -8px;
    margin: 0 3px 0 2px;
    font-size: 12px;
  }
  .post h3 {
    margin-top: 48px;
    font-size: 20px;
  }
  .post a {
    cursor: pointer;
  }

</style>

<div class="post">
  <div class="intro w-page">
    <h1>Experiments in Handwriting with a Neural Network</h1>
    <h2>Some are fun, like the one below that tries to predict your strokes as you write. Some are more serious and attempt to divine meaning from the model.</h2>
  </div>

  <div style="position: relative;">
    <div style="position: absolute; width: 100%;">
      <div class="controls w-page" id="figure-predict-controls" style="text-align: center;z-index: 10;position: relative;margin-top: 18px;">
        <button onclick="document.querySelector('figure-predict').playToggle()">Play/Pause</button>
        <button onclick="document.querySelector('figure-predict').clear()">Clear</button>
        <button onclick="document.querySelector('figure-predict').setTest()">Set</button>
        <span style="margin-left: 18px;">Length of prediction</span>
        <input type="range" value="20" min="5" max="200" step="5" oninput="document.querySelector('figure-predict').setPredictionLength(this.value); document.querySelector('#figure-predict-controls .prediction-length').textContent = this.value;" style="width: 70px;"/>
        <span class="prediction-length" style="display: inline-block; width: 18px; text-align: left; margin-left: 6px;">20</span>

        <span style="margin-left: 18px;">Variation<sup><a href="#footnote1">1</a></sup></span>
        <input type="range" value="0.1" min="0.1" max="0.7" step="0.05" oninput="document.querySelector('figure-predict').setTemperature(this.value); document.querySelector('#figure-predict-controls .temperature').textContent = this.value;" style="width: 70px;"/>
        <span class="temperature" style="display: inline-block; width: 40px;">0.1</span>

      </div>
    </div>
    <figure-predict class="w-screen" temperature="0.1" predictionlength="20"></figure-predict>
  </div>

  {{> byline.html}}

  <p> Neural networks are an extremely successful approach to machine learning, but it’s tricky to understand why they behave the way they do. This has sparked a lot of interest and effort around trying to understand and visualize them, which we think is so far just scratching the surface of what is possible.</p>

  <p>In this article we will try to push forward in this direction by taking a generative model of handwriting<sup><a href="#footnote2">2</a></sup> and visualizing it in a number of ways. In the end we don’t have some ultimate answer or visualization, but we do have some interesting ideas to share. Ultimately we hope they make it easier to divine some meaning from the internals of these model.</p>

  <h3>Looking at the Output of the Model</h3>

  <p>Our first experiment is the most obvious: when we want to see how well someone has learned a task we usually ask them to demonstrate it. So, let’s ask our model to write something for us and see how well it does.</p>

  <div class="controls w-body" id="figure-animated-controls">
    <button class="play" onclick="document.querySelector('figure-animated').togglePlay()">Play/Pause</button>
    <span style="margin-left: 8px;">Variation<sup><a href="#footnote1">1</a></sup></span>
    <input type="range" value="0.6" min="0.1" max="1.5" step="0.05" oninput="document.querySelector('figure-animated').temperature(this.value); document.querySelector('#figure-animated-controls .value').textContent = this.value;" />
    <span class="value">0.6</span>
  </div>
  <figure-animated class="w-page" temperature="0.6"></figure-animated>

  <!-- <p>To get the model to generate that handwriting, we simply fed in a starting point and then asked it to predict the next position of the pen. We then feed this sample back into the model and repeated the process over and over. After a few hundred iterations we have a sample of what the model thinks is a credible handwriting example.</p> -->

  <!-- <div class="controls w-body">
    <button class="play" onclick="document.querySelector('figure-gaussian').play()">Play/Pause</button>
    <button class="restart" onclick="document.querySelector('figure-gaussian').restart()">Restart</button>
  </div>
  <figure-gaussian class="w-page"></figure-gaussian> -->

  <p>Most of the marks are gibberish, but many of them are surprisingly convincing. Some real (or real-ish) words even start to appear. One surprising thing you’ll notice is that the general style of handwriting is more or less consistent within a sample. This is because the type of architecture used for this model (LSTM) has a mechanism for remembering previous strokes. It is therefore able to remember things like how loopy or jerky, or which letter preceeded the current one. (For more on LSTMs and how they can remember, Chris Olah has a <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">good primer</a>.)</p>

  <p>Even with this memory, the technique for generating samples from neural networks is probabilistic, meaning each of these samples is one of a much, much larger possibility space. Viewing them one at a time feels unsatisfying — how can we infer much from one or two samples when there are nearly infinite possibilities? If something looks wrong, how do we know if it was because there is something fundamentally wrong with our model or if it was just dumb luck?</p>

  <figure class="p-right-margin" style="width: 145px;">
    <img class="key" src="assets/color-key.svg">
  </figure>

  <p>At each iteration our model could have produced many paths. Instead of picking one and throwing the rest away, let’s draw, say, 50 of them. Green strokes below are places where the model would have veered rightward from the chosen stroke, orange is where it would have veered leftward.</p>

  <figure-hairs class="w-screen">
    <div class="controls w-body">
      <select class="sample">
        <option value="001">Generated sample 1</option>
        <option value="002">Generated sample 2</option>
        <option value="003" selected>Generated sample 3</option>
        <option value="004">Generated sample 4</option>
        <option value="005">Generated sample 5</option>
        <option value="straight">Straight line</option>
      </select>

      <span style="margin-left: 18px;">Steps</span>
      <input type="range" min="0" max="2" step="1" class="step" value="2" style="width: 80px;"></input>
      <span class="value">4</span>

      <span style="margin-left: 18px;">Variation<sup><a href="#footnote1">1</a></sup></span>
      <input type="range" min="0" max="2" step="1" class="temperature" value="1" style="width: 80px;"></input>
      <span class="temperature-val">0.5</span>
    </div>
  </figure-hairs>

  <p>With this technique we are casting a light into a wider possibility space. You can see some areas where there was general consensus as to what to do next. Other areas had more of a “anything can happen next” feeling. Others seem to show a possible fork in the road. “We can be drawing an “a” or “g”. A few steps later it’s clear the model has converged on a cursive “g”.</p>


  <figure class="p-right-margin" style="width: 145px;">
    <img class="key" src="assets/color-key.svg">
  </figure>

  <p>In addition to visualizing samples generated by the model, this technique can also be applied to human-generated samples. Below we can see what the model would have done at each point if it had taken over for the person.</p>

  <figure-hairs class="w-screen">
    <div class="controls w-body">
      <select class="sample">
        <option value="0">Validation sample 0</option>
        <option value="1">Validation sample 1</option>
        <option value="2" selected>Validation sample 2</option>
        <option value="3">Validation sample 3</option>
        <option value="4">Validation sample 4</option>
        <option value="v7">Validation sample 5</option>
        <option value="v5">Validation sample 6</option>
      </select>

      <span style="margin-left: 18px;">Steps</span>
      <input type="range" min="0" max="2" step="1" class="step" value="2" style="width: 80px;"></input>
      <span class="value">4</span>

      <span style="margin-left: 18px;">Variation<sup><a href="#footnote1">1</a></sup></span>
      <input type="range" min="0" max="2" step="1" class="temperature" value="1" style="width: 80px;"></input>
      <span class="temperature-val">0.5</span>
    </div>
  </figure-hairs>

  <!-- <p>[normalized handwriting]</p>

  <figure-normalize class="w-screen"></figure-normalize> -->

  <p>It is obvious from these experiments that this model has learned quite a lot about human handwriting. Which sort of raises the question, can we extract that knowledge in any meaningful way, rather than just blindly using it to mimic handwriting?</p>

  <h3>Examining the Internals of the Model</h3>

  <p>Our model has 500 cells which act as a sort of memory which it will use as part of its input when deciding what to generate. If we can see what those cells are doing as the model progresses we may be able to gain some intuitive understanding about what the model is doing.</p>

  <p>We begin by showing the activation of the cells over time. Each column in the heatmap below represents one line segment of the handwriting. Each row represents one cell of the model and is colored by its activations on that part of the stroke. By inspecting the diagram you may be able to see some patterns in the way certain cells activate for certain types of strokes. You can change which cell is used to color the strokes by clicking on the diagram.</p>

  <figure-cells class="w-page" mode="unsorted" defaults="93,0"></figure-cells>

  <p>You may have been able to pick out one or two cells with interesting patterns, but we have reason to believe that cells work in tandem, such that an individual cell’s activity may not be as interesting as a group of cells.</p>

  <p>Is there a way to order the cells to make this structure clearer? We've found that applying one-dimensional <a href="https://lvdmaaten.github.io/tsne/">t-SNE</a> to the activations of the cells over time can organize them, bringing cells with similar behavior close together. This makes the diagram easier to read. We use a few small tricks to achieve the best results.<sup><a href="#footnote3">3</a></sup></p>

  <figure-cells class="w-page" mode="sorted" defaults="71,0"></figure-cells>

  <p>In this version we can find a few clear behaviors. For example, many of the cells at the top, such as cell <a onclick="cellClick(45)">25</a> are sensitive to slightly different directions of the pen. Other cells track absolute position. Cell <a onclick="cellClick(64)">64</a> and its neighbors seem concerned with horizontal position, while cells around <a onclick="cellClick(236)">236</a> seem concerned with vertical position. Cell <a onclick="cellClick(242)">242</a> tracks position within a word. Many cells towards the bottom seem concerned with where the pen lifts up; for example, cell <a onclick="cellClick(494)">494</a> seems to predict whether the pen is about to be lifted.</p>

  <!--<p>In this version we can find a few clear behaviors. For example, cells <a onclick="cellClick(45)">45</a>, <a onclick="cellClick(277)">277</a> and <a onclick="cellClick(286)">286</a> seem be focused on the pen lifting off the page, or transitions between letters. Cells <a onclick="cellClick(253)">253</a>, <a onclick="cellClick(262)">262</a> and <a onclick="cellClick(275)">275</a> seem to activate sharply at the start of a new letter. Cells <a onclick="cellClick(60)">60</a> and <a onclick="cellClick(61)">61</a> appear to ramp up just before the transition between letters. Cell <a onclick="cellClick(117)">117</a> shows a smooth, horizontal gradient, implying that it is remembering the x position of the pen, or perhaps the length of the sample. Some cells seem to track directionality of the strokes, for example cells <a onclick="cellClick(20)">20</a>,<a onclick="cellClick(21)">21</a> and <a onclick="cellClick(22)">22</a> activate strongly on strokes that move up and to the right while cell <a onclick="cellClick(30)">30</a> activates on flat right strokes. Cell <a onclick="cellClick(71)">71</a> oscillates between positive and negative activations corresponding with upwards and downwards directions. Cell <a onclick="cellClick(35)">35</a> looks like it keeps track of whether a stroke is at the top or bottom of the character, perhaps pushing the pen up and down.-->

  <!--<p>In this version we can find a few clear behaviors. For example, cells 112-127 seem be focused on the pen lifting off the page, or transitions between letters. Cells 340-350 seem to also be focused on pen change states, but almost seem to be negative mirror images. Cell 117 shows a smooth, horizontal gradient, implying that it is remembering the x position of the pen, or perhaps the length of the sample.
  -->

  <p>Another way to explore the activations is to give it a sample and interactively see the activations. Below you can write and see the activations of all the cells in real time.</p>

  <div class="controls w-page" id="figure-cells-draw-controls" style="margin-top: 36px;">
    <button onclick="document.querySelector('figure-cells-draw').clear()">Clear</button>
  </div>
  <figure-cells-draw class="w-page" mode="sorted" defaults="71,0"></figure-cells-draw>

  <h3>Conclusion</h3>

  <p>The black box reputation of machine learning models is well deserved, but we believe part of that reputation has been born from the programming context into which they have been locked into. The experience of having an easily inspectable model available in the same programming context as the interactive visualization environment (here, javascript) proved to be very productive for prototyping and exploring new ideas for this post.</p>

  <p>As we are able to move them more and more into the same programming context that user interface work is done, we believe we will see richer modes of human-ai interactions flourish. This could have a marked impact on debugging and building models, for sure, but also in how the models are used. Machine learning research typically seeks to mimic and substitute humans, and increasingly it’s able to. What seems less explored is using machine learning to augment humans. This sort of complicated human-machine interaction is best explored when the full capabilities of the model are available in the user interface context.</p>

</div>

<section class="appendix">

  <h3>Footnotes</h3>

  <p><a id="footnote1">1.</a> The model has a parameter which determines how widely it samples from the underlying distribution. It is labeled here as variation but is more commonly referred to as temperature. Temperature is most commonly discussed in <a href="https://en.wikipedia.org/wiki/Boltzmann_distribution">Boltzmann distributions</a>, but can be generalized to all probability distributions. In this more general form, changing the temperature by a factor of <i>T</i> corresponds to raising all probabilities to the power of <i>1/T</i> and normalizing.</p>

  <p><a id="footnote2">2.</a> The model used in this articles is a version of the model described in Section 4.2 of <a href="https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Network</a> by Alex Graves. It is a small LSTM, with 500 hidden units, trained to perform the unconditional handwriting generation task.  For a detailed description of the model and training procedure, please refer to <a href="http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/">this blog</a> post in addition to the Graves paper.  After training the LSTM, we quantized the weights using 8-bit integers, and exported the weights into a small <a href="https://jb64.org/specification/">JSON-Base64</a> formatted file.</p>

  <p><a id="footnote3">3.</a> We use two different tricks to make the t-SNE organization of neurons work better. First, if a cell state is mostly negative, we canonicalize it by flipping its sign. (From the LSTM's perspective, this is completely equivelant, provided we flip the sign of some weights. The sign of a cell state is arbitrary.) Secondly, we blur cell states slightly over time before using t-SNE to organize them. This causes t-SNE to see cells that lag or lead eachother slightly as close together.</p>


  <h3>Acknowledgments</h3>

  <p>We are grateful for the comments of Fernanda Viegas, Martin Wattenberg, and Daniel Smilkov.

  <p>This work was made possible by the support of the <a href="https://research.google.com/teams/brain/">Google Brain</a> team.

  <h3>Author Contributions</h3>

  <p>Shan Carter wrote the article and created the interactive experiments in the first section. David Ha created the handwriting model and ported it to Javascript. Ian Johnson created the final diagrams exploring activations. Chris Olah provided guidance and core ideas for the diagrams and edited the article.</p>

  <h3 id="citation">Errors, Reuse, and Citation</h3>
  <p>If you see mistakes or want to suggest changes, please submit a pull request on <a href="{{{distill.github}}}">github</a>.
  <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY 2.0</a>, unless noted otherwise, with the source available on <a href="{{{distill.github}}}">github</a>.
  <p>For attribution in academic contexts, please cite this work as
  <pre class="citation">{{distill.concatenatedAuthors}} "{{distill.title}}", Distill, {{distill.firstPublishedYear}}.</pre>
  <p>BibTeX citation
  {{=<% %>=}}
<pre class="citation">@misc{<%distill.slug%>,
  author = {<%distill.bibtexAuthors%>},
  title = {<%distill.title%>},
  year = {<%distill.firstPublishedYear%>},
  howpublished = {<%distill.url%>}
}</pre>

  <%={{ }}=%>
  <h3>References</h3>
  <ul class="references">
    <li><a href="https://arxiv.org/pdf/1308.0850v5.pdf">Graves, A., 2013. <b>Generating sequences with recurrent neural networks.</b> arXiv preprint arXiv:1308.0850.</a></li>
    <li><a href="">Ha, D., 2016. <b>Handwriting Generation Demo in TensorFlow</b>.</a></li>
    <li><a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf">Maaten, L.V.D. and Hinton, G., 2008. <b>Visualizing data using t-SNE.</b> Journal of Machine Learning Research, 9(Nov), pp.2579-2605.</a></li>
    <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Olah, C., 2015. <b>Understanding LSTM Networks</b>.</a></li>
  </ul>

</section>
